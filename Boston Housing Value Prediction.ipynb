{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set random number generator with constant seed. Ensures we compare model consistently\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 13)\n",
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "# Import complete dataset\n",
    "from sklearn.datasets import load_boston\n",
    "features,target = load_boston(return_X_y=True)\n",
    "Y = target.reshape(506,1)\n",
    "X = features.astype(float)\n",
    "#Limit tuning with limited rows to ensure code runs quickly\n",
    "X_train=X[1:101,:]\n",
    "Y_train=Y[1:101,:]\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic single layer model without tuning parameters\n",
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = KerasRegressor(build_fn=baseline_model, nb_epoch=100, batch_size=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 57.82 (42.25) MSE\n"
     ]
    }
   ],
   "source": [
    "# Evaluae basic single layer model without tuning parameters\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameter tuning using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tuning optimization algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model to obtain best optimizer\n",
    "def create_model_optimizer(optimizer='SGD'):\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "# As a rule of thumb, no. of neurons in hidden layer is kept same as no. of input features\n",
    "\tmodel.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = KerasRegressor(build_fn=create_model_optimizer, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the grid search parameters - Optimization Algorithms\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "param_grid = dict(optimizer=optimizer)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, scoring='neg_mean_squared_error')\n",
    "grid_result = grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -70.252350 using {'optimizer': 'Adagrad'}\n",
      "-1210191.495685 (1015514.115262) with: {'optimizer': 'SGD'}\n",
      "-264.188642 (146.012773) with: {'optimizer': 'RMSprop'}\n",
      "-70.252350 (21.798538) with: {'optimizer': 'Adagrad'}\n",
      "-340.369199 (200.028205) with: {'optimizer': 'Adadelta'}\n",
      "-188.814646 (52.591622) with: {'optimizer': 'Adam'}\n",
      "-108.382188 (59.600982) with: {'optimizer': 'Adamax'}\n",
      "-213.236959 (160.906641) with: {'optimizer': 'Nadam'}\n"
     ]
    }
   ],
   "source": [
    "# Summarize results - Optimization Algorithms\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tuning Epoch and Batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model to optain best Epoch and Batchsize\n",
    "def create_model_epoch_batchsize():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='Adagrad')\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = KerasRegressor(build_fn=create_model_epoch_batchsize, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the grid search parameters - Epochs and Batch Size\n",
    "batch_size = [1, 3 , 5]\n",
    "epochs = [200, 400, 500]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, scoring='neg_mean_squared_error')\n",
    "grid_result = grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -20.364575 using {'epochs': 500, 'batch_size': 3}\n",
      "-22.109959 (4.581594) with: {'epochs': 200, 'batch_size': 1}\n",
      "-23.964450 (3.626423) with: {'epochs': 400, 'batch_size': 1}\n",
      "-22.562409 (5.993306) with: {'epochs': 500, 'batch_size': 1}\n",
      "-20.669693 (4.790279) with: {'epochs': 200, 'batch_size': 3}\n",
      "-22.360193 (3.535647) with: {'epochs': 400, 'batch_size': 3}\n",
      "-20.364575 (4.420059) with: {'epochs': 500, 'batch_size': 3}\n",
      "-20.961925 (6.086711) with: {'epochs': 200, 'batch_size': 5}\n",
      "-21.101111 (3.962289) with: {'epochs': 400, 'batch_size': 5}\n",
      "-23.337207 (5.679260) with: {'epochs': 500, 'batch_size': 5}\n"
     ]
    }
   ],
   "source": [
    "# Summarize results - Epochs and Batch Size\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tuning Learning Rate\n",
    "# Adagrad doesn't have momentum as an hyper parameter. So we look only for Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model to optain best Learning Rate\n",
    "def create_model_learningRate(learn_rate=0.1):\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(1,kernel_initializer='normal'))\n",
    "\t# Compile model\n",
    "\toptimizer = Adagrad(lr=learn_rate)\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = KerasRegressor(build_fn=create_model_learningRate, epochs=500, batch_size=3, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the grid search parameters - Learning Rate\n",
    "learn_rate = [0.1, 0.075, 0.06]\n",
    "param_grid = dict(learn_rate=learn_rate)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, scoring='neg_mean_squared_error')\n",
    "grid_result = grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -16.810857 using {'learn_rate': 0.1}\n",
      "-16.810857 (4.082318) with: {'learn_rate': 0.1}\n",
      "-24.376610 (8.526254) with: {'learn_rate': 0.075}\n",
      "-27.136983 (3.028903) with: {'learn_rate': 0.06}\n"
     ]
    }
   ],
   "source": [
    "# Summarize results - Learning Rate\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tuning Kernal Initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model to optain best Kernal Initializer\n",
    "def create_model_init(init_mode='uniform'):\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(13, input_dim=13, kernel_initializer=init_mode, activation='relu'))\n",
    "\tmodel.add(Dense(1, kernel_initializer=init_mode))\n",
    "\t# Compile model\n",
    "\toptimizer = Adagrad(lr=0.1)\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = KerasRegressor(build_fn=create_model_init, epochs=500, batch_size=3, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the grid search parameters - Kernal Initializer\n",
    "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "param_grid = dict(init_mode=init_mode)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, scoring='neg_mean_squared_error')\n",
    "grid_result = grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -22.710047 using {'init_mode': 'uniform'}\n",
      "-22.710047 (6.437737) with: {'init_mode': 'uniform'}\n",
      "-50.615799 (27.585039) with: {'init_mode': 'lecun_uniform'}\n",
      "-39.134081 (25.993987) with: {'init_mode': 'normal'}\n",
      "-72.045191 (22.079562) with: {'init_mode': 'zero'}\n",
      "-25.417910 (8.388176) with: {'init_mode': 'glorot_normal'}\n",
      "-28.496305 (7.501855) with: {'init_mode': 'glorot_uniform'}\n",
      "-39.720868 (22.390881) with: {'init_mode': 'he_normal'}\n",
      "-25.305118 (1.867289) with: {'init_mode': 'he_uniform'}\n"
     ]
    }
   ],
   "source": [
    "# Summarize results - Kernal Initializer\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tuning Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model to optain best Activation function\n",
    "def create_model_activation(activation='softmax'):\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(13, input_dim=13, kernel_initializer='uniform', activation=activation))\n",
    "\tmodel.add(Dense(1, kernel_initializer='uniform'))\n",
    "\t# Compile model\n",
    "\toptimizer = Adagrad(lr=0.1)\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = KerasRegressor(build_fn=create_model_activation, epochs=500, batch_size=3, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the grid search parameters - Activation function\n",
    "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "param_grid = dict(activation=activation)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, scoring='neg_mean_squared_error')\n",
    "grid_result = grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -15.623848 using {'activation': 'softsign'}\n",
      "-39.875107 (8.515270) with: {'activation': 'softmax'}\n",
      "-40.768370 (26.455653) with: {'activation': 'softplus'}\n",
      "-15.623848 (3.964667) with: {'activation': 'softsign'}\n",
      "-30.994681 (13.384043) with: {'activation': 'relu'}\n",
      "-40.888661 (11.598617) with: {'activation': 'tanh'}\n",
      "-40.742364 (11.609105) with: {'activation': 'sigmoid'}\n",
      "-40.744912 (11.583928) with: {'activation': 'hard_sigmoid'}\n",
      "-21.720744 (7.037041) with: {'activation': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# Summarize results - Activation function\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tuning Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.constraints import maxnorm\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model to optain best Droput Regularization\n",
    "def create_model_weightRegularization(dropout_rate=0.0, weight_constraint=0):\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(13, input_dim=13, kernel_initializer='uniform', activation='softsign', kernel_constraint=maxnorm(weight_constraint)))\n",
    "\tmodel.add(Dropout(dropout_rate))\n",
    "\tmodel.add(Dense(1, kernel_initializer='uniform'))\n",
    "\t# Compile model\n",
    "\toptimizer = Adagrad(lr=0.1)\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = KerasRegressor(build_fn=create_model_weightRegularization, epochs=500, batch_size=3, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the grid search parameters - Dropout Regularization\n",
    "weight_constraint = [1, 2, 3, 4, 5]\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "param_grid = dict(dropout_rate=dropout_rate, weight_constraint=weight_constraint)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, scoring='neg_mean_squared_error')\n",
    "grid_result = grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -14.406803 using {'dropout_rate': 0.1, 'weight_constraint': 2}\n",
      "-17.805588 (4.512625) with: {'dropout_rate': 0.0, 'weight_constraint': 1}\n",
      "-16.308607 (4.748015) with: {'dropout_rate': 0.0, 'weight_constraint': 2}\n",
      "-20.491957 (10.771632) with: {'dropout_rate': 0.0, 'weight_constraint': 3}\n",
      "-22.948010 (4.417483) with: {'dropout_rate': 0.0, 'weight_constraint': 4}\n",
      "-20.002946 (6.896428) with: {'dropout_rate': 0.0, 'weight_constraint': 5}\n",
      "-19.564888 (7.021127) with: {'dropout_rate': 0.1, 'weight_constraint': 1}\n",
      "-14.406803 (4.420103) with: {'dropout_rate': 0.1, 'weight_constraint': 2}\n",
      "-19.047971 (6.350501) with: {'dropout_rate': 0.1, 'weight_constraint': 3}\n",
      "-20.343022 (11.022341) with: {'dropout_rate': 0.1, 'weight_constraint': 4}\n",
      "-22.215568 (8.599333) with: {'dropout_rate': 0.1, 'weight_constraint': 5}\n",
      "-18.685232 (3.958685) with: {'dropout_rate': 0.2, 'weight_constraint': 1}\n",
      "-20.402755 (7.756556) with: {'dropout_rate': 0.2, 'weight_constraint': 2}\n",
      "-21.318868 (3.774191) with: {'dropout_rate': 0.2, 'weight_constraint': 3}\n",
      "-20.152535 (7.569969) with: {'dropout_rate': 0.2, 'weight_constraint': 4}\n",
      "-20.930563 (10.190143) with: {'dropout_rate': 0.2, 'weight_constraint': 5}\n",
      "-26.926667 (5.403102) with: {'dropout_rate': 0.3, 'weight_constraint': 1}\n",
      "-28.686872 (7.984424) with: {'dropout_rate': 0.3, 'weight_constraint': 2}\n",
      "-23.603608 (4.856685) with: {'dropout_rate': 0.3, 'weight_constraint': 3}\n",
      "-22.716257 (5.629262) with: {'dropout_rate': 0.3, 'weight_constraint': 4}\n",
      "-24.152760 (6.027552) with: {'dropout_rate': 0.3, 'weight_constraint': 5}\n",
      "-28.893996 (7.542863) with: {'dropout_rate': 0.4, 'weight_constraint': 1}\n",
      "-24.715024 (5.329491) with: {'dropout_rate': 0.4, 'weight_constraint': 2}\n",
      "-24.425149 (6.908088) with: {'dropout_rate': 0.4, 'weight_constraint': 3}\n",
      "-25.389544 (8.375427) with: {'dropout_rate': 0.4, 'weight_constraint': 4}\n",
      "-24.868602 (4.503760) with: {'dropout_rate': 0.4, 'weight_constraint': 5}\n",
      "-26.248380 (6.536317) with: {'dropout_rate': 0.5, 'weight_constraint': 1}\n",
      "-30.775548 (8.205824) with: {'dropout_rate': 0.5, 'weight_constraint': 2}\n",
      "-27.858810 (5.883990) with: {'dropout_rate': 0.5, 'weight_constraint': 3}\n",
      "-26.541341 (8.475087) with: {'dropout_rate': 0.5, 'weight_constraint': 4}\n",
      "-25.910556 (8.955218) with: {'dropout_rate': 0.5, 'weight_constraint': 5}\n",
      "-31.417611 (9.742019) with: {'dropout_rate': 0.6, 'weight_constraint': 1}\n",
      "-32.171945 (6.226173) with: {'dropout_rate': 0.6, 'weight_constraint': 2}\n",
      "-28.573661 (6.505965) with: {'dropout_rate': 0.6, 'weight_constraint': 3}\n",
      "-31.729852 (9.806606) with: {'dropout_rate': 0.6, 'weight_constraint': 4}\n",
      "-28.692609 (5.944365) with: {'dropout_rate': 0.6, 'weight_constraint': 5}\n",
      "-28.687052 (8.608678) with: {'dropout_rate': 0.7, 'weight_constraint': 1}\n",
      "-31.293702 (8.969302) with: {'dropout_rate': 0.7, 'weight_constraint': 2}\n",
      "-32.611434 (8.573632) with: {'dropout_rate': 0.7, 'weight_constraint': 3}\n",
      "-34.506005 (6.903650) with: {'dropout_rate': 0.7, 'weight_constraint': 4}\n",
      "-29.780674 (7.343688) with: {'dropout_rate': 0.7, 'weight_constraint': 5}\n",
      "-39.543313 (8.568133) with: {'dropout_rate': 0.8, 'weight_constraint': 1}\n",
      "-38.661935 (12.379463) with: {'dropout_rate': 0.8, 'weight_constraint': 2}\n",
      "-34.372907 (11.373918) with: {'dropout_rate': 0.8, 'weight_constraint': 3}\n",
      "-36.845263 (9.152346) with: {'dropout_rate': 0.8, 'weight_constraint': 4}\n",
      "-35.950416 (12.867933) with: {'dropout_rate': 0.8, 'weight_constraint': 5}\n",
      "-49.968389 (12.091988) with: {'dropout_rate': 0.9, 'weight_constraint': 1}\n",
      "-48.736285 (15.672972) with: {'dropout_rate': 0.9, 'weight_constraint': 2}\n",
      "-47.298418 (13.854620) with: {'dropout_rate': 0.9, 'weight_constraint': 3}\n",
      "-45.641938 (12.833499) with: {'dropout_rate': 0.9, 'weight_constraint': 4}\n",
      "-49.382300 (18.333128) with: {'dropout_rate': 0.9, 'weight_constraint': 5}\n"
     ]
    }
   ],
   "source": [
    "# Summarize results - Dropout Regularization\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tuning No. of Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model to optain best no. of neurons\n",
    "def create_model_neurons(neurons=1):\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(neurons, input_dim=13, kernel_initializer='uniform', activation='softsign', kernel_constraint=maxnorm(2)))\n",
    "\tmodel.add(Dropout(0.1))\n",
    "\tmodel.add(Dense(1, kernel_initializer='uniform'))\n",
    "\t# Compile model\n",
    "\toptimizer = Adagrad(lr=0.1)\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = KerasRegressor(build_fn=create_model_neurons, epochs=500, batch_size=3, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the grid search parameters - No. of neurons\n",
    "neurons = [5, 10, 15, 20, 25, 30]\n",
    "param_grid = dict(neurons=neurons)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, scoring='neg_mean_squared_error')\n",
    "grid_result = grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -13.830474 using {'neurons': 25}\n",
      "-30.164018 (9.190192) with: {'neurons': 5}\n",
      "-20.192496 (6.956323) with: {'neurons': 10}\n",
      "-17.994083 (5.175047) with: {'neurons': 15}\n",
      "-16.983643 (5.959624) with: {'neurons': 20}\n",
      "-13.830474 (3.903831) with: {'neurons': 25}\n",
      "-15.500631 (4.090846) with: {'neurons': 30}\n"
     ]
    }
   ],
   "source": [
    "# Summarize results - No. of neurons\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Base model with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model_base():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(25, input_dim=13, kernel_initializer='uniform', activation='softsign', kernel_constraint=maxnorm(2)))\n",
    "\tmodel.add(Dropout(0.1))\n",
    "\tmodel.add(Dense(1, kernel_initializer='uniform'))\n",
    "\t# Compile model\n",
    "\toptimizer = Adagrad(lr=0.1)\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 32.74 (26.01) MSE\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model with complete dataset\n",
    "model = KerasRegressor(build_fn=create_model_base, epochs=500, batch_size=3, verbose=0)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The above result shows decrease in error as we tuned the parameters\n",
    "\n",
    "# Standardizing Scales\n",
    "# The features in Boston dataset are all numerical. \n",
    "# The scales of each feature in the dataset is different. \n",
    "# It is a good practice to normalize the features to get a more precise model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 33.63 (42.42) MSE\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model with standardized dataset\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=create_model_base, epochs=500, batch_size=3, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deeper model\n",
    "\n",
    "# Theoretically, we can expect an increase in performace as we increase the number of layers.\n",
    "# Our model might learn higher order features as we increase layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 22.35 (28.52) MSE\n"
     ]
    }
   ],
   "source": [
    "def create_larger():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(25, input_dim=13, kernel_initializer='uniform', activation='softsign', kernel_constraint=maxnorm(2)))\n",
    "\tmodel.add(Dropout(0.1))\n",
    "\tmodel.add(Dense(10, kernel_initializer='uniform', activation='softsign'))\n",
    "\tmodel.add(Dense(5, kernel_initializer='uniform', activation='softsign'))\n",
    "\tmodel.add(Dense(1, kernel_initializer='uniform'))\n",
    "\t# Compile model\n",
    "\toptimizer = Adagrad(lr=0.1)\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\treturn model\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=create_larger, epochs=500, batch_size=3, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The deeper model has certainly lesser error than our base model\n",
    "\n",
    "# Wider model\n",
    "\n",
    "# Another approach to build a model with increased performance capability is to increase the number of neurons\n",
    "# In general, the performance of Deeper model is better than Wider model\n",
    "# In some cases, Wider model might perform better. Hence, it is essential to always try with different combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 30.07 (31.71) MSE\n"
     ]
    }
   ],
   "source": [
    "def create_wider():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(35, input_dim=13, kernel_initializer='uniform', activation='softsign', kernel_constraint=maxnorm(2)))\n",
    "\tmodel.add(Dropout(0.1))\n",
    "\tmodel.add(Dense(1, kernel_initializer='uniform'))\n",
    "\t# Compile model\n",
    "\toptimizer = Adagrad(lr=0.1)\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\treturn model\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=create_wider, epochs=500, batch_size=3, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The wider model performs better than standardized baseline model, but is not as good as Deeper model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
